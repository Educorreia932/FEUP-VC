{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic sign detection and classification (with deep learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import xml.etree.ElementTree as ET\n",
    "from utils import ModelTrainer, displayImage, importImage, plotTrainingHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and validation datasets\n",
    "images_directory = \"../dataset/images\"\n",
    "annotations_directory = \"../dataset/annotations\"\n",
    "\n",
    "with open(\"train.txt\") as train:\n",
    "    train_images_filenames = sorted(train.read().splitlines())\n",
    "\n",
    "with open(\"test.txt\") as test:\n",
    "    val_images_filenames = sorted(test.read().splitlines())\n",
    "\n",
    "# Filter out images that can not be loaded properly\n",
    "train_images_filenames = [i for i in train_images_filenames if cv.imread(os.path.join(images_directory, i + \".png\")) is not None]\n",
    "val_images_filenames = [i for i in val_images_filenames if cv.imread(os.path.join(images_directory, i + \".png\")) is not None]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(train_images_filenames)\n",
    "random.shuffle(val_images_filenames)\n",
    "\n",
    "print(len(train_images_filenames), len(val_images_filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    \"trafficlight\": 0,\n",
    "    \"stop\": 1,\n",
    "    \"speedlimit\": 2,\n",
    "    \"crosswalk\": 3,\n",
    "}\n",
    "\n",
    "class TrafficSignDataset(Dataset):\n",
    "    def __init__(self, annotations_directory, images_filenames, images_directory, transform=None):\n",
    "        self.annotations_directory = annotations_directory\n",
    "        self.images_filenames = images_filenames\n",
    "        self.images_directory = images_directory\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.images_filenames[idx]\n",
    "        image = cv.imread(os.path.join(\n",
    "            self.images_directory, image_filename + \".png\"))\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "        # transform image into 0-1 range\n",
    "        # note that the ToTensorV2 method from the albumentations library does not automatically convert the image range into 0-1\n",
    "        image = image / 255.\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed[\"image\"]\n",
    "\n",
    "        return image.float(), self._get_max_area_class_from_annotion(image_filename)\n",
    "\n",
    "    def _get_max_area_class_from_annotion(self, filename):\n",
    "        with open(os.path.join(self.annotations_directory, filename + \".xml\")) as xml:\n",
    "            tree = ET.parse(xml)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            max_area_class = \"trafficlight\"\n",
    "            max_area = -1\n",
    "\n",
    "            objects = root.findall(\"object\")\n",
    "            for object in objects:\n",
    "                xmin = int(object.find(\"bndbox/xmin\").text)\n",
    "                ymin = int(object.find(\"bndbox/ymin\").text)\n",
    "                xmax = int(object.find(\"bndbox/xmax\").text)\n",
    "                ymax = int(object.find(\"bndbox/ymax\").text)\n",
    "\n",
    "                area = (xmax - xmin) * (ymax - ymin)\n",
    "\n",
    "                if area > max_area:\n",
    "                    max_area = area\n",
    "                    max_area_class = object.find(\"name\").text\n",
    "\n",
    "        return classes[max_area_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_workers = 2 # How many processes are used to load the data\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(256, 256),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "train_dataset = TrafficSignDataset(annotations_directory, train_images_filenames, images_directory, transform=train_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [A.Resize(256, 256), ToTensorV2()]\n",
    ")\n",
    "val_dataset = TrafficSignDataset(annotations_directory, val_images_filenames, images_directory, transform=val_transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "\n",
    "index = 10\n",
    "print(train_images_filenames[index])\n",
    "# displayImage(train_dataset[index][0])\n",
    "print(train_dataset[index][1])\n",
    "displayImage(importImage(train_images_filenames[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(classes)\n",
    "\n",
    "# Get CPU or GPU device for training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-50 pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = models.resnet50(pretrained=True)\n",
    "# change the number of neurons in the last layer to the number of classes of the problem at hand (CIFAR10 dataset)\n",
    "resnet_model.fc = nn.Linear(in_features=2048, out_features=num_classes, bias=True)\n",
    "resnet_model.to(device)\n",
    "\n",
    "resnet = {\n",
    "    \"model\": resnet_model,\n",
    "    \"name\": 'resnet50',\n",
    "    \"num_epochs\": 50,\n",
    "    \"loss\": nn.CrossEntropyLoss(),  # already includes the Softmax activation\n",
    "    \"optimizer\": torch.optim.SGD(resnet_model.parameters(), lr=1e-3)\n",
    "}\n",
    "\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_trainer = ModelTrainer(resnet, device)\n",
    "resnet_train_history, resnet_val_history = resnet_trainer.train(train_dataloader, val_dataloader)\n",
    "\n",
    "plotTrainingHistory(resnet_train_history, resnet_val_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_trainer.visualize_model(val_dataloader, classes, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from torchvision (with pretrained=True)\n",
    "vgg_model = models.vgg16(pretrained=True)\n",
    "# Change the number of neurons in the last layer to the number of classes of the problem at hand (CIFAR10 dataset)\n",
    "vgg_model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "vgg_model.to(device)\n",
    "\n",
    "vgg = {\n",
    "    \"model\": vgg_model,\n",
    "    \"name\": 'vgg16',\n",
    "    \"num_epochs\": 10,\n",
    "    \"loss\": nn.CrossEntropyLoss(),  # already includes the Softmax activation\n",
    "    \"optimizer\": torch.optim.SGD(vgg_model.parameters(), lr=1e-3)\n",
    "}\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only train last layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_trainer = ModelTrainer(vgg, device)\n",
    "vgg_train_history, vgg_val_history = vg_trainer.train(train_dataloader, val_dataloader)\n",
    "\n",
    "plotTrainingHistory(vgg_train_history, vgg_val_history)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd9946677ea18cd89e211de7c948dc8a1318cc7d7b55887506aa41faca795d54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
