{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import IPython\n",
    "#app = IPython.Application.instance()\n",
    "#app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # Adapt adp-get to your distro\n",
    "# !pip install -U jupyter\n",
    "# !apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y python3-opencv unzip\n",
    "# !pip install opencv-python torchvision torchaudio albumentations kaggle tdqm torchsummary torchmetrics\n",
    "# !pip install --upgrade --quiet jupyter_client ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!mkdir -p ~/.kaggle\n",
    "#!echo \"{\\\"username\\\":\\\"vcguy2022\\\",\\\"key\\\":\\\"58a013ff978771be5b85417b3ee3917c\\\"}\"  > /root/.kaggle/kaggle.json\n",
    "#!kaggle datasets download andrewmvd/road-sign-detection\n",
    "#!unzip road-sign-detection -d dataset/;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import xml.etree.ElementTree as ET\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490 123 264\n"
     ]
    }
   ],
   "source": [
    "# Get train and validation datasets\n",
    "images_directory = \"dataset/images\"\n",
    "annotations_directory = \"dataset/annotations\"\n",
    "\n",
    "train_split = 0.8\n",
    "\n",
    "with open(\"train.txt\") as train:\n",
    "    train_images_filenames_total = train.read().splitlines()\n",
    "\n",
    "    split_idx = int(train_split * len(train_images_filenames_total))\n",
    "    train_images_filenames = train_images_filenames_total[:split_idx]\n",
    "    val_images_filenames = train_images_filenames_total[split_idx:]\n",
    "\n",
    "\n",
    "with open(\"test.txt\") as test:\n",
    "    test_images_filenames = test.read().splitlines()\n",
    "\n",
    "# Filter out images that can not be loaded properly\n",
    "train_images_filenames = [i for i in train_images_filenames if cv.imread(os.path.join(images_directory, i + \".png\")) is not None]\n",
    "val_images_filenames = [i for i in val_images_filenames if cv.imread(os.path.join(images_directory, i + \".png\")) is not None]\n",
    "test_images_filenames = [i for i in test_images_filenames if cv.imread(os.path.join(images_directory, i + \".png\")) is not None]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(train_images_filenames)\n",
    "random.shuffle(val_images_filenames)\n",
    "random.shuffle(test_images_filenames)\n",
    "\n",
    "print(len(train_images_filenames), len(val_images_filenames), len(test_images_filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to YOLO annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import shutil\n",
    "# classes = {\n",
    "#     \"trafficlight\": 0,\n",
    "#     \"stop\": 1,\n",
    "#     \"speedlimit\": 2,\n",
    "#     \"crosswalk\": 3,\n",
    "# }\n",
    "\n",
    "# os.makedirs(\"dataset/labels\", exist_ok=True)\n",
    "\n",
    "# os.makedirs(\"dataset/images/train\", exist_ok=True)\n",
    "# os.makedirs(\"dataset/images/val\", exist_ok=True)\n",
    "# os.makedirs(\"dataset/images/test\", exist_ok=True)\n",
    "# os.makedirs(\"dataset/labels/train\", exist_ok=True)\n",
    "# os.makedirs(\"dataset/labels/val\", exist_ok=True)\n",
    "# os.makedirs(\"dataset/labels/test\", exist_ok=True)\n",
    "\n",
    "\n",
    "# for img in train_images_filenames:\n",
    "#     shutil.copy(\n",
    "#         f\"dataset/images/{img}.png\", f\"dataset/images/train/{img}.png\")\n",
    "\n",
    "# for img in val_images_filenames:\n",
    "#     shutil.copy(\n",
    "#         f\"dataset/images/{img}.png\", f\"dataset/images/val/{img}.png\")\n",
    "\n",
    "# for img in test_images_filenames:\n",
    "#     shutil.copy(\n",
    "#         f\"dataset/images/{img}.png\", f\"dataset/images/test/{img}.png\")\n",
    "\n",
    "\n",
    "# for path in os.listdir(annotations_directory):\n",
    "#     with open(os.path.join(annotations_directory, path), \"r\") as xml:\n",
    "#         folder = \"\"\n",
    "#         if path.strip(\".xml\") in train_images_filenames:\n",
    "#             folder = \"train\"\n",
    "#         elif path.strip(\".xml\") in val_images_filenames:\n",
    "#             folder = \"val\"\n",
    "#         else:\n",
    "#             folder = \"test\"\n",
    "\n",
    "\n",
    "#         with open(f\"dataset/labels/{folder}/{path.replace('xml', 'txt')}\", \"w\") as txt:\n",
    "#             tree = ET.parse(xml)\n",
    "#             root = tree.getroot()\n",
    "\n",
    "#             size = root.find(\"size\")\n",
    "#             height = int(size.find(\"height\").text)\n",
    "#             width = int(size.find(\"width\").text)\n",
    "\n",
    "#             objects = root.findall(\"object\")\n",
    "\n",
    "#             lines = []\n",
    "#             for object in objects:\n",
    "#                 class_index = classes[object.find(\"name\").text]\n",
    "\n",
    "#                 xmin = int(object.find(\"bndbox/xmin\").text)\n",
    "#                 ymin = int(object.find(\"bndbox/ymin\").text)\n",
    "#                 xmax = int(object.find(\"bndbox/xmax\").text)\n",
    "#                 ymax = int(object.find(\"bndbox/ymax\").text)\n",
    "\n",
    "#                 # middle of bbox\n",
    "#                 bbox_x = ((xmax + xmin) / 2) / width\n",
    "#                 bbox_y = ((ymax + ymin) / 2) / height\n",
    "#                 bbox_width = (xmax - xmin) / width\n",
    "#                 bbox_height = (ymax - ymin) / height\n",
    "\n",
    "#                 lines.append(\n",
    "#                     f\"{class_index} {bbox_x} {bbox_y} {bbox_width} {bbox_height}\")\n",
    "\n",
    "#             txt.write(\"\\n\".join(lines))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    \"trafficlight\": 1,\n",
    "    \"stop\": 2,\n",
    "    \"speedlimit\": 3,\n",
    "    \"crosswalk\": 4,\n",
    "}\n",
    "\n",
    "\n",
    "class TrafficSignDataset(Dataset):\n",
    "    def __init__(self, annotations_directory, images_filenames, images_directory, transform=None):\n",
    "        self.annotations_directory = annotations_directory\n",
    "        self.images_filenames = images_filenames\n",
    "        self.images_directory = images_directory\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.images_filenames[idx]\n",
    "        image = cv.imread(os.path.join(\n",
    "            self.images_directory, image_filename + \".png\"))\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "        image = image / 255.\n",
    "\n",
    "        boxes, labels = self._get_boxes_and_labels(image_filename)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(\n",
    "                image=image, bboxes=target[\"boxes\"], labels=target[\"labels\"])\n",
    "            image = transformed[\"image\"]\n",
    "            target[\"boxes\"] = transformed[\"bboxes\"]\n",
    "            target[\"labels\"] = transformed[\"labels\"]\n",
    "\n",
    "            if len(target[\"boxes\"]) == 0:\n",
    "                height, width, channels = image.shape\n",
    "                target[\"boxes\"] = torch.as_tensor(\n",
    "                    [[0, 0, width, height]], dtype=torch.float32)\n",
    "                target[\"labels\"] = torch.as_tensor([0], dtype=torch.int64)\n",
    "\n",
    "            target[\"boxes\"] = torch.as_tensor(\n",
    "                target[\"boxes\"], dtype=torch.float32)\n",
    "            target[\"labels\"] = torch.as_tensor(\n",
    "                target[\"labels\"], dtype=torch.int64)\n",
    "\n",
    "        return image.float(), target\n",
    "\n",
    "    # https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "    def _get_boxes_and_labels(self, filename):\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        with open(os.path.join(self.annotations_directory, filename + \".xml\")) as xml:\n",
    "            tree = ET.parse(xml)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            objects = root.findall(\"object\")\n",
    "            for object in objects:\n",
    "                class_index = classes[object.find(\"name\").text]\n",
    "\n",
    "                xmin = int(object.find(\"bndbox/xmin\").text)\n",
    "                ymin = int(object.find(\"bndbox/ymin\").text)\n",
    "                xmax = int(object.find(\"bndbox/xmax\").text)\n",
    "                ymax = int(object.find(\"bndbox/ymax\").text)\n",
    "\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                labels.append(class_index)\n",
    "\n",
    "        return boxes, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(classes) + 1\n",
    "\n",
    "# Get CPU or GPU device for training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 0  # how many processes are used to load the data\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(256, 256),\n",
    "        A.RandomSizedBBoxSafeCrop(224, 224),\n",
    "        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2,\n",
    "                           rotate_limit=30, p=0.5),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=['labels'])\n",
    ")\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [A.Resize(256, 256), A.RandomSizedBBoxSafeCrop(224, 224), ToTensorV2()],\n",
    "    bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=['labels'])\n",
    ")\n",
    "\n",
    "train = TrafficSignDataset(annotations_directory,\n",
    "                           train_images_filenames, images_directory, train_transform)\n",
    "val = TrafficSignDataset(annotations_directory,\n",
    "                         val_images_filenames, images_directory, val_transform)\n",
    "test = TrafficSignDataset(annotations_directory,\n",
    "                          test_images_filenames, images_directory)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True, collate_fn=lambda batch: tuple(zip(*batch)))\n",
    "val_dataloader = DataLoader(val, batch_size=batch_size,\n",
    "                            shuffle=False, num_workers=num_workers, drop_last=False, collate_fn=lambda batch: tuple(zip(*batch)))\n",
    "test_dataloader = DataLoader(\n",
    "    test, batch_size=1, shuffle=False, num_workers=num_workers, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (0): ConvNormActivation(\n",
       "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): ConvNormActivation(\n",
       "            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): ConvNormActivation(\n",
       "            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): ConvNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): ConvNormActivation(\n",
       "            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): ConvNormActivation(\n",
       "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): ConvNormActivation(\n",
       "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): ConvNormActivation(\n",
       "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(160, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(\n",
    "    pretrained=True)\n",
    "model.backbone.requires_grad_(False)\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# from torchmetrics import JaccardIndex\n",
    "# metric = JaccardIndex(num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = \"models\"\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, *args):\n",
    "        if len(args) == 5:\n",
    "            model, model_name, optimizer, device = args\n",
    "        elif len(args) == 2:\n",
    "            ks = [\"model\", \"name\", \"num_epochs\", \"optimizer\"]\n",
    "            model, model_name, num_epochs, optimizer = [\n",
    "                args[0][k] for k in ks]\n",
    "            device = args[1]\n",
    "\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.curr_epoch = 0\n",
    "        self.num_epochs = num_epochs  # Epochs that each training session will have\n",
    "        self.scheduler = None\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.metrics = {}\n",
    "\n",
    "    def set_scheduler(self, scheduler, gamma):\n",
    "        if scheduler != None:\n",
    "            self.scheduler = scheduler(self.optimizer, gamma)\n",
    "\n",
    "    def predict_data(self, data):\n",
    "        # TODO change\n",
    "        print(\"Predicting data\")\n",
    "        preds = []\n",
    "        actuals = []\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for _, (X, y) in enumerate(tqdm(data)):\n",
    "                pred = self.model(X)\n",
    "                probs = F.softmax(pred, dim=1)\n",
    "                final_pred = torch.argmax(probs, dim=1)\n",
    "                preds.extend(final_pred)\n",
    "                actuals.extend(y)\n",
    "        return torch.stack(preds, dim=0), torch.stack(actuals, dim=0)\n",
    "\n",
    "    def _epoch_iter(self, dataloader):\n",
    "\n",
    "        num_batches = len(dataloader)\n",
    "        self.model.train()  # put model in train mode\n",
    "\n",
    "        total_loss = 0.0\n",
    "        with torch.set_grad_enabled(True):\n",
    "            for images, targets in tqdm(dataloader):\n",
    "                images = list(image.to(self.device) for image in images)\n",
    "                targets = [{k: v.to(self.device) for k, v in t.items()}\n",
    "                           for t in targets]\n",
    "\n",
    "                # Compute loss\n",
    "                loss_dict = self.model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                losses.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Save training metrics\n",
    "                # IMPORTANT: call .item() to obtain the value of the loss WITHOUT the computational graph attached\n",
    "                total_loss += losses.item()\n",
    "\n",
    "        return total_loss / num_batches\n",
    "\n",
    "    def _eval(self, dataloader, metric):\n",
    "        model.eval()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for images, targets in tqdm(dataloader):\n",
    "                images = list(image.to(self.device) for image in images)\n",
    "\n",
    "                outputs = self.model(images)\n",
    "                outputs = [{k: v.to(\"cpu\") for k, v in t.items()}\n",
    "                           for t in outputs]\n",
    "\n",
    "                # res = {target[\"image_id\"].item(): output for target,\n",
    "                #        output in zip(targets, outputs)}\n",
    "\n",
    "                metric.update(outputs, targets)\n",
    "\n",
    "        return metric.compute()\n",
    "\n",
    "    def _save_model(self, t, file_name):\n",
    "        import os\n",
    "        os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "        model_path = os.path.join(\n",
    "            MODELS_DIR, f'{self.model_name}_{ file_name }.pth')\n",
    "\n",
    "        save_dict = {'model': self.model.state_dict(\n",
    "        ), 'optimizer': self.optimizer.state_dict(), 'epoch': t}\n",
    "        torch.save(save_dict, model_path)\n",
    "\n",
    "        metrics_path = os.path.join(\n",
    "            MODELS_DIR, f'{self.model_name}_metrics_history.json')\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            f.write(json.dumps(self.metrics))\n",
    "\n",
    "    def load_model(self, file_name):\n",
    "        model_path = os.path.join(\n",
    "            MODELS_DIR, f'{self.model_name}_{ file_name }.pth')\n",
    "        metrics_path = os.path.join(\n",
    "            MODELS_DIR, f'{self.model_name}_metrics_history.json')\n",
    "        dic = torch.load(model_path)\n",
    "        self.model.load_state_dict(dic['model'])\n",
    "        self.model.eval()\n",
    "\n",
    "        with open(metrics_path, 'r') as f:\n",
    "            self.metrics = json.load(f)\n",
    "\n",
    "        print(f\"Loaded { self.model_name } obtained in epoch { dic['epoch'] }\")\n",
    "\n",
    "    def append_history(self, stage, train_data, val_data):\n",
    "        self.metrics[stage] = {\n",
    "            \"train\": {\n",
    "                \"loss\": train_data\n",
    "            },\n",
    "            \"val\": {\n",
    "\n",
    "            }\n",
    "        }\n",
    "        for k in val_data.keys():\n",
    "            self.metrics[stage][\"val\"][k] = val_data[k].tolist()\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        self.metrics = {}\n",
    "\n",
    "    def train(self, train_dataloader, validation_dataloader, stage):\n",
    "        # Stage => String identifier of the training iteration. Is used to identify between\n",
    "        # the same model being trained at different contexts (ex. fine tunning)\n",
    "\n",
    "        metric = MeanAveragePrecision(\n",
    "            box_format=\"xyxy\", iou_type=\"bboxes\", class_metrics=True)\n",
    "\n",
    "        best_val_meanap = np.inf\n",
    "        print(\"Start training...\")\n",
    "\n",
    "        for _ in range(self.num_epochs):\n",
    "            t = self.curr_epoch\n",
    "            print(f\"\\nEpoch {t}\")\n",
    "\n",
    "            # Train\n",
    "            train_loss = self._epoch_iter(train_dataloader)\n",
    "            print(f\"Train loss: {train_loss:.3f}\")\n",
    "\n",
    "            # Validation\n",
    "            val_meanap = self._eval(validation_dataloader, metric)\n",
    "            print(\"Mean Average Precision:\")\n",
    "            print(val_meanap)\n",
    "\n",
    "            # save training history for plotting purposes\n",
    "            self.append_history(f'{stage}_{t}', (train_loss), (val_meanap))\n",
    "\n",
    "            # Save model when validation loss improves\n",
    "            if val_meanap[\"map\"] < best_val_meanap:\n",
    "                best_val_meanap = val_meanap[\"map\"]\n",
    "                self._save_model(f'{stage}_{t}', 'best_model')\n",
    "\n",
    "            # Save latest model\n",
    "            self._save_model(f'{stage}_{t}', 'latest_model')\n",
    "            self.curr_epoch += 1\n",
    "\n",
    "            print(self.metrics)\n",
    "\n",
    "        print(\"Finished\")\n",
    "\n",
    "    def freeze_layers(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_layers(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def unfreeze_head(self):\n",
    "        self.model.roi_heads.requires_grad = True\n",
    "\n",
    "    # Adapted from https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "    def visualize_model(self, val_dataloader, classes, num_images=6):\n",
    "        was_training = self.model.training\n",
    "        self.model.eval()\n",
    "        images_so_far = 0\n",
    "        fig = plt.figure()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(val_dataloader):\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                for j in range(inputs.size()[0]):\n",
    "                    images_so_far += 1\n",
    "                    ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                    ax.axis('off')\n",
    "                    class_names = list(classes.keys())\n",
    "                    ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
    "                    cv.imshow(inputs.cpu().data[j])\n",
    "\n",
    "                    if images_so_far == num_images:\n",
    "                        self.model.train(mode=was_training)\n",
    "                        return\n",
    "            self.model.train(mode=was_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(\n",
    "    pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "frcnn = {\n",
    "    \"model\": model,\n",
    "    \"name\": 'frcnn',\n",
    "    \"num_epochs\": 10,\n",
    "    \"optimizer\": optimizer\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frcnn_trainer = ModelTrainer(frcnn, device)\n",
    "frcnn_trainer.scheduler = lr_scheduler\n",
    "# Unfreeze all layers expect last\n",
    "# frcnn_trainer.freeze_layers()\n",
    "# frcnn_trainer.unfreeze_head()\n",
    "model.backbone.requires_grad_(False)\n",
    "\n",
    "\n",
    "def load_model(trainer):  # Returns True if success\n",
    "    import os\n",
    "    path = os.path.join(MODELS_DIR, trainer.model_name + '_best_model.pth')\n",
    "    file_exists = os.path.exists(path)\n",
    "    print(trainer.model_name + '_best_model.pth')\n",
    "    if not file_exists:\n",
    "        print(\"Saved model not found, training instead.\")\n",
    "        return False\n",
    "    else:\n",
    "        frcnn.load_model(\"best_model\")\n",
    "        return True\n",
    "\n",
    "\n",
    "# Training can be skipped by loading the best model until now\n",
    "load = False  # Change this to False to force to retrain\n",
    "if load:\n",
    "    need_train = (load_model(frcnn_trainer) == False)\n",
    "if not load or need_train:\n",
    "    frcnn_trainer.train(train_dataloader, val_dataloader, \"frozen\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ultralytics/yolov5\n",
    "\n",
    "# !pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might need to be run on linux/WSL\n",
    "# !python3 yolov5/train.py --batch 10 --epochs 30 --data trafficsigns.yaml\n",
    "# python yolov5/train.py --batch -1 --epochs 3 --data trafficsigns.yaml --workers 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection\n",
    "# !python3 yolov5/detect.py --weights yolov5/runs/train/exp22/weights/best.pt --img 640 --conf 0.25 --source dataset/images/train/road2.png"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
